{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955a78fe-1a35-4ed6-b801-ad8f43188f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc9ff155-f4ad-41e5-a678-dc639e320c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-8eca9400641949349ba17730270b006b\"  \n",
    "\n",
    "# 替换模型初始化\n",
    "llm = ChatOpenAI(model=\"qwen-max-0428\",\n",
    "                base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340ae6a8-13d6-4aec-92a2-cfbd37ad9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph as Py2NeoGraph\n",
    "from typing import List, Optional, Dict\n",
    "from graph_chain import build_graph_qa_chain, get_node_names, build_entities_dict\n",
    "\n",
    "uri = \"neo4j+s://a36f6471.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = \"E7QyiOiiDAi0SjQY7eXvUyjNPNzEHa4sYsMdlTqc1gI\"\n",
    "\n",
    "file_path = \"entity_cache.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    entity_cache = json.load(f)\n",
    "\n",
    "graph = Py2NeoGraph(uri, auth=(username, password))\n",
    "\n",
    "graph_qa_chain = build_graph_qa_chain(llm, graph, entity_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b60ae7a-1810-4900-8c05-501f3fd452b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_type': 'Entity-Type', 'database_to_call': 'Literature Graph Database', 'first_database_to_call': 'N/A', 'methods': None, 'call_strategy': 'Which papers have been published on kerr solitons since 2024?'}\n"
     ]
    }
   ],
   "source": [
    "from determine_database import determine_database\n",
    "\n",
    "question = \"Which papers have been published on kerr solitons in since 2024?\"\n",
    "\n",
    "parsed_output, response = determine_database(question, llm)\n",
    "\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "43ee1381-9bd5-49bf-b2ee-5ed756156ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"'Aura connected!'\": 'Aura connected!'}]\n",
      "Authors=[] Papers=[] Years=[2024] Sources=[] Keywords=['kerr solitons']\n",
      "{'Authors': [], 'Papers': [], 'Years': [2024], 'Sources': [], 'Keywords': ['Kerr Solitons']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\lib\\site-packages\\pydantic\\main.py:1114: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query=\"MATCH (p:Paper)-[:PUBLISHED_IN_YEAR]->(y:Year), (p)-[:KEYWORD]->(kw:Keyword) WHERE kw.name = 'Kerr Solitons' AND y.year = 2024 RETURN p.title AS Title, y.year AS Year\"\n",
      "[{'Title': 'Photorefraction-Assisted Self-Emergence of Dissipative Kerr Solitons', 'Year': 2024}, {'Title': 'Atom-referenced and stabilized soliton microcomb', 'Year': 2024}, {'Title': 'Optimizing auxiliary laser heating for Kerr soliton microcomb generation', 'Year': 2024}, {'Title': 'Repetition rate tuning and locking of solitons in a microrod resonator', 'Year': 2024}, {'Title': 'Turn-key Kerr soliton generation and tunable microwave synthesizer in dual-mode Si3N4 microresonators', 'Year': 2024}, {'Title': 'Mechanically Actuated Kerr Soliton Microcombs', 'Year': 2024}, {'Title': 'Octave-spanning Kerr soliton frequency combs in dispersion- and dissipation-engineered lithium niobate microresonators', 'Year': 2024}, {'Title': 'Ultralow-Noise K-Band Soliton Microwave Oscillator Using Optical Frequency Division', 'Year': 2024}, {'Title': 'Active feedback stabilization of super-efficient microcombs in photonic molecules', 'Year': 2024}, {'Title': 'Progress in Optical Frequency Combs Based on Non-integrated Microresonators(Invited); [非 集 成 微 腔 中 的 光 学 频 率 梳 研 究 进 展（特 邀）]', 'Year': 2024}, {'Title': 'Optical Frequency Domain Reflectometry using Dual Kerr Soliton Microcombs', 'Year': 2024}, {'Title': '3 THz Frequency Synthesizer with Hz Stability', 'Year': 2024}, {'Title': 'Kerr solitons generation in cylindrical microresonators', 'Year': 2024}, {'Title': 'Generation of self-stabilized chirped dissipative Kerr solitons in the normal-dispersion regime of a Si3 N4 microring resonator with built-in spectral filtering', 'Year': 2024}, {'Title': 'Dissipative Kerr soliton generation at 2µm in a silicon nitride microresonator', 'Year': 2024}, {'Title': 'Generation of Kerr Soliton Frequency Comb in an On-Chip Microresonator Assisted by Raman Scattering', 'Year': 2024}, {'Title': 'Frequency-offset Kerr soliton comb generation in a dispersion-shifted fiber Fabry-Perot resonator', 'Year': 2024}, {'Title': 'Microresonator Frequency Combs Involving Both x (2) and x (3) Nonlinearities', 'Year': 2024}, {'Title': 'Soliton Frequency Combs via Cascaded Brillouin Scattering', 'Year': 2024}, {'Title': 'Analysis and optimization of optical frequency comb spectra of magnesium fluoride microbottle resonator*; [氟化镁微瓶腔光频梳光谱分析及优化]', 'Year': 2024}, {'Title': 'Soliton optical frequency comb generation from polygon modes in weakly perturbed lithium niobate microdisks', 'Year': 2024}, {'Title': 'Mitigation of thermal instabilities in Kerr microcombs', 'Year': 2024}]\n"
     ]
    }
   ],
   "source": [
    "from agent import agent, answer_visualize\n",
    "\n",
    "huggingface_key = \"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\"\n",
    "wcd_url = \"https://aisrbu5qqwmbaknbgpcja.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
    "wcd_api_key = \"lKvgU6NgrDQ5WXfI818jyRok6ddMOfhBeQAL\"\n",
    "\n",
    "question_type = parsed_output['question_type']\n",
    "result = agent(llm, question, parsed_output, wcd_url, wcd_api_key, huggingface_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d203b2-763d-4251-8301-e87e9ba7ffd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who wrote Coherent optical communications using coherence-cloned Kerr soliton microcombs?', 'extract': QuestionEntities(Authors=[], Papers=['Coherent optical communications using coherence-cloned Kerr soliton microcombs'], Years=[], Sources=[], Keywords=[]), 'entities': QuestionEntities(Authors=[], Papers=['Coherent optical communications using coherence-cloned Kerr soliton microcombs'], Years=[], Sources=[], Keywords=[]), 'cypher': \"MATCH (a:Author)-[:WROTE]->(p:Paper {title: 'Coherent optical communications using coherence-cloned Kerr soliton microcombs'}) RETURN a.name AS author_name\", 'context': [{'author_name': 'Zhou, Heng'}, {'author_name': 'Geng, Yong'}, {'author_name': 'Cui, Wenwen'}, {'author_name': 'Zhou, Qiang'}, {'author_name': 'Qiu, Kun'}, {'author_name': 'Han, Xinjie'}, {'author_name': 'Zhang, Qiang'}, {'author_name': 'Liu, Boyuan'}, {'author_name': 'Deng, Guangwei'}], 'answer': 'The paper \"Coherent optical communications using coherence-cloned Kerr soliton microcombs\" was written by the following authors: Zhou, Heng; Geng, Yong; Cui, Wenwen; Zhou, Qiang; Qiu, Kun; Han, Xinjie; Zhang, Qiang; Liu, Boyuan; and Deng, Guangwei.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fab01735-e40d-4e1c-8778-0617bf0e726b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e812f1b2-ed31-43e9-9545-bd69bfc0e3a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import re\n",
    "\n",
    "# 类型识别函数（模糊识别）\n",
    "def infer_node_type(field_name: str) -> str:\n",
    "    field = field_name.lower()\n",
    "    if \"paper\" in field or \"title\" in field:\n",
    "        return \"Papers\"\n",
    "    elif \"author\" in field:\n",
    "        return \"Authors\"\n",
    "    elif \"keyword\" in field:\n",
    "        return \"Keywords\"\n",
    "    elif \"year\" in field:\n",
    "        return \"Years\"\n",
    "    elif \"source\" in field or \"journal\" in field or \"conference\" in field:\n",
    "        return \"Sources\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# 关系推断函数\n",
    "def infer_relationship(source_type: str, target_type: str) -> str:\n",
    "    source_type, target_type = source_type.capitalize(), target_type.capitalize()\n",
    "    if source_type == \"Authors\" and target_type == \"Papers\":\n",
    "        return \"WROTE\"\n",
    "    elif source_type == \"Papers\" and target_type == \"Authors\":\n",
    "        return \"WROTE\"\n",
    "    elif source_type == \"Papers\" and target_type == \"Papers\":\n",
    "        return \"CITES\"\n",
    "    elif source_type == \"Papers\" and target_type == \"Years\":\n",
    "        return \"PUBLISHED_IN_YEAR\"\n",
    "    elif source_type == \"Years\" and target_type == \"Papers\":\n",
    "        return \"PUBLISHED_IN_YEAR\"\n",
    "    elif source_type == \"Papers\" and target_type == \"Sources\":\n",
    "        return \"PUBLISHED_IN_SOURCE\"\n",
    "    elif source_type == \"Sources\" and target_type == \"Papers\":\n",
    "        return \"PUBLISHED_IN_SOURCE\"\n",
    "    elif source_type == \"Papers\" and target_type == \"Keywords\":\n",
    "        return \"KEYWORD\"\n",
    "    return \"\"\n",
    "\n",
    "# 添加引用关系的辅助函数\n",
    "def fetch_and_add_citation_edges(G: nx.DiGraph, graph, paper_titles: List[str]):\n",
    "    if len(paper_titles) < 2:\n",
    "        return\n",
    "    titles_str = ','.join(f'\"{title}\"' for title in paper_titles)\n",
    "    query = f\"\"\"\n",
    "    MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "    WHERE p1.title IN [{titles_str}] AND p2.title IN [{titles_str}]\n",
    "    RETURN p1.title AS source, p2.title AS target\n",
    "    \"\"\"\n",
    "    records = graph.run(query).data()\n",
    "    for rec in records:\n",
    "        src_id = f\"Papers:{rec['source']}\"\n",
    "        tgt_id = f\"Papers:{rec['target']}\"\n",
    "        if not G.has_edge(src_id, tgt_id):\n",
    "            G.add_edge(src_id, tgt_id, label=\"CITES\")\n",
    "\n",
    "# 主函数\n",
    "def build_complete_graph(context_result: List[Dict[str, Any]], corrected_entities: Dict[str, List[str]], graph) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    added_nodes = set()\n",
    "\n",
    "    # Step 1: 添加 corrected_entities 节点\n",
    "    for ent_type, values in corrected_entities.items():\n",
    "        for val in values:\n",
    "            node_id = f\"{ent_type}:{val}\"\n",
    "            if node_id not in added_nodes:\n",
    "                G.add_node(node_id, label=ent_type, name=val)\n",
    "                added_nodes.add(node_id)\n",
    "\n",
    "    # Step 2: 添加 context_result 中的节点与内部边\n",
    "    for record in context_result:\n",
    "        typed_nodes = []\n",
    "        for key, val in record.items():\n",
    "            if not val:\n",
    "                continue\n",
    "            node_type = infer_node_type(key)\n",
    "            node_id = f\"{node_type}:{val}\"\n",
    "            if node_id not in added_nodes:\n",
    "                G.add_node(node_id, label=node_type, name=val)\n",
    "                added_nodes.add(node_id)\n",
    "            typed_nodes.append((node_type, val))\n",
    "\n",
    "        if len(typed_nodes) >= 2:\n",
    "            for i in range(len(typed_nodes)):\n",
    "                for j in range(i + 1, len(typed_nodes)):\n",
    "                    src_type, src_val = typed_nodes[i]\n",
    "                    tgt_type, tgt_val = typed_nodes[j]\n",
    "                    src_id, tgt_id = f\"{src_type}:{src_val}\", f\"{tgt_type}:{tgt_val}\"\n",
    "                    rel = infer_relationship(src_type, tgt_type)\n",
    "                    if not G.has_edge(src_id, tgt_id):\n",
    "                        G.add_edge(src_id, tgt_id, label=rel)\n",
    "\n",
    "    # Step 3: 连接 corrected_entities 和 context_result\n",
    "    for ent_type, values in corrected_entities.items():\n",
    "        for val in values:\n",
    "            ent_id = f\"{ent_type}:{val}\"\n",
    "            for record in context_result:\n",
    "                for k, v in record.items():\n",
    "                    node_type = infer_node_type(k)\n",
    "                    node_id = f\"{node_type}:{v}\"\n",
    "                    if str(v) == str(val) and ent_id == node_id:\n",
    "                        continue  # 已经存在该节点\n",
    "                    if not v or ent_id == node_id:\n",
    "                        continue\n",
    "                    if node_type in [\"Year\", \"Source\"]:\n",
    "                        # 只允许 Paper -> Year/Source 一次\n",
    "                        if any((edge[0] == ent_id and G[edge[0]][edge[1]]['label'] == f\"PUBLISHED_IN_{node_type.upper()}\") for edge in G.edges):\n",
    "                            continue\n",
    "                    if ent_id not in G:\n",
    "                        G.add_node(ent_id, label=ent_type, name=val)\n",
    "                    if node_id not in G:\n",
    "                        G.add_node(node_id, label=node_type, name=v)\n",
    "                    rel = infer_relationship(ent_type, node_type)\n",
    "                    if not G.has_edge(ent_id, node_id):\n",
    "                        G.add_edge(ent_id, node_id, label=rel)\n",
    "\n",
    "    # Step 4: 添加 Paper 节点间的引用关系\n",
    "    paper_titles = [n.split(\":\", 1)[1] for n in G.nodes if G.nodes[n][\"label\"] == \"Papers\"]\n",
    "    fetch_and_add_citation_edges(G, graph, paper_titles)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "context_result = result['context']\n",
    "corrected_entities = result['entities'].dict()\n",
    "\n",
    "G = build_complete_graph(context_result, corrected_entities, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9821ac43-5603-4116-b6f9-c3b6089ae36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图中的边数为: 70\n"
     ]
    }
   ],
   "source": [
    "num_edges = G.number_of_edges()\n",
    "print(f\"图中的边数为: {num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6688418f-975f-442f-b72a-c9e62275ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_graph(G, output_path=\"graph1.html\"):\n",
    "    net = Network(height=\"800px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "    \n",
    "    net.set_options(\"\"\"\n",
    "        var options = {\n",
    "          \"nodes\": {\n",
    "            \"font\": {\n",
    "              \"size\": 20,\n",
    "              \"face\": \"arial\",\n",
    "              \"color\": \"#000000\"\n",
    "            },\n",
    "            \"physics\": true\n",
    "          },\n",
    "          \"interaction\": {\n",
    "            \"dragNodes\": true\n",
    "          },\n",
    "          \"manipulation\": {\n",
    "            \"enabled\": false\n",
    "          },\n",
    "          \"physics\": {\n",
    "            \"enabled\": true,\n",
    "            \"solver\": \"forceAtlas2Based\",\n",
    "            \"forceAtlas2Based\": {\n",
    "              \"gravitationalConstant\": -50,\n",
    "              \"centralGravity\": 0.01,\n",
    "              \"springLength\": 120,\n",
    "              \"springConstant\": 0.08,\n",
    "              \"avoidOverlap\": 1\n",
    "            },\n",
    "            \"minVelocity\": 0.75,\n",
    "            \"stabilization\": {\n",
    "              \"enabled\": true,\n",
    "              \"iterations\": 1000,\n",
    "              \"updateInterval\": 25\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "    for node_id, data in G.nodes(data=True):\n",
    "        net.add_node(\n",
    "            node_id,\n",
    "            label=data[\"name\"],\n",
    "            title=data[\"label\"],\n",
    "            group=data[\"label\"],\n",
    "            size=20,\n",
    "            font={\"size\": 20, \"color\": \"black\"}\n",
    "        )\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        net.add_edge(u, v, label=data[\"label\"])\n",
    "\n",
    "    net.write_html(output_path)\n",
    "\n",
    "\n",
    "visualize_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "253a1e4e-274a-4348-837a-3a91529b0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_graph(G, output_path=\"graph.html\"):\n",
    "    net = Network(height=\"800px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "    \n",
    "    net.set_options(\"\"\"\n",
    "        var options = {\n",
    "          \"nodes\": {\n",
    "            \"font\": {\n",
    "              \"size\": 20,\n",
    "              \"face\": \"arial\",\n",
    "              \"color\": \"#000000\"\n",
    "            },\n",
    "            \"physics\": true\n",
    "          },\n",
    "          \"interaction\": {\n",
    "            \"dragNodes\": true\n",
    "          },\n",
    "          \"manipulation\": {\n",
    "            \"enabled\": false\n",
    "          },\n",
    "          \"physics\": {\n",
    "            \"enabled\": true,\n",
    "            \"solver\": \"forceAtlas2Based\",\n",
    "            \"forceAtlas2Based\": {\n",
    "              \"gravitationalConstant\": -50,\n",
    "              \"centralGravity\": 0.01,\n",
    "              \"springLength\": 120,\n",
    "              \"springConstant\": 0.08,\n",
    "              \"avoidOverlap\": 1\n",
    "            },\n",
    "            \"minVelocity\": 0.75,\n",
    "            \"stabilization\": {\n",
    "              \"enabled\": true,\n",
    "              \"iterations\": 1000,\n",
    "              \"updateInterval\": 25\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "    for node_id, data in G.nodes(data=True):\n",
    "        net.add_node(\n",
    "            node_id,\n",
    "            label=str(data[\"name\"]),\n",
    "            title=data[\"label\"],\n",
    "            group=data[\"label\"],\n",
    "            size=20,\n",
    "            font={\"size\": 20, \"color\": \"black\"}\n",
    "        )\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        net.add_edge(u, v, label=data[\"label\"])\n",
    "\n",
    "    net.write_html(output_path)\n",
    "\n",
    "\n",
    "visualize_graph(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0b53c54b-73d9-405c-a7c8-8f3bf9bfe7e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def fuzzy_node_type(key: str) -> str:\n",
    "    key_lower = key.lower()\n",
    "    if \"paper\" in key_lower or \"title\" in key_lower:\n",
    "        return \"Papers\"\n",
    "    elif \"author\" in key_lower:\n",
    "        return \"Authors\"\n",
    "    elif \"year\" in key_lower:\n",
    "        return \"Years\"\n",
    "    elif \"source\" in key_lower or \"journal\" in key_lower or \"conference\" in key_lower:\n",
    "        return \"Sources\"\n",
    "    elif \"keyword\" in key_lower or \"key\" in key_lower:\n",
    "        return \"Keywords\"\n",
    "    else:\n",
    "        return key.capitalize()\n",
    "\n",
    "\n",
    "RELATIONSHIP_MAP = {\n",
    "    (\"Authors\", \"Papers\"): \"WROTE\",\n",
    "    (\"Papers\", \"Authors\"): \"WROTE\",\n",
    "    (\"Papers\", \"Papers\"): \"CITES\",\n",
    "    (\"Papers\", \"Years\"): \"PUBLISHED_IN_YEAR\",\n",
    "    (\"Years\", \"Papers\"): \"PUBLISHED_IN_YEAR\",\n",
    "    (\"Papers\", \"Sources\"): \"PUBLISHED_IN_SOURCE\",\n",
    "    (\"Sources\", \"Papers\"): \"PUBLISHED_IN_SOURCE\",\n",
    "    (\"Papers\", \"Keywords\"): \"KEYWORD\",\n",
    "    (\"Keywords\", \"Papers\"): \"KEYWORD\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_relationship_and_direction(label1, label2):\n",
    "    \"\"\"\n",
    "    返回关系名称和边的方向 (from_label, to_label)\n",
    "    \"\"\"\n",
    "    if (label1, label2) in RELATIONSHIP_MAP:\n",
    "        return RELATIONSHIP_MAP[(label1, label2)], (label1, label2)\n",
    "    elif (label2, label1) in RELATIONSHIP_MAP:\n",
    "        return RELATIONSHIP_MAP[(label2, label1)], (label2, label1)\n",
    "    else:\n",
    "        return \"RELATED\", (label1, label2)\n",
    "\n",
    "\n",
    "def context_and_entities_to_graph(context_result, corrected_entities, graph):\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    added_nodes = set()\n",
    "\n",
    "    # --- Step 1: 添加 corrected_entities 的实体节点 ---\n",
    "    for entity_type, values in corrected_entities.items():\n",
    "        for value in values:\n",
    "            node_id = f\"{entity_type}:{value}\"\n",
    "            if node_id not in added_nodes:\n",
    "                G.add_node(node_id, label=entity_type, name=value)\n",
    "                added_nodes.add(node_id)\n",
    "\n",
    "    # --- Step 2: 添加 context_result 中的节点和上下文内部边 ---\n",
    "    for record in context_result:\n",
    "        keys = list(record.keys())\n",
    "        # 先把record中所有字段节点都加上（去重）\n",
    "        for key in keys:\n",
    "            val = record[key]\n",
    "            if not val:\n",
    "                continue\n",
    "            label = fuzzy_node_type(key)\n",
    "            node_id = f\"{label}:{val}\"\n",
    "            if node_id not in added_nodes:\n",
    "                G.add_node(node_id, label=label, name=val)\n",
    "                added_nodes.add(node_id)\n",
    "\n",
    "        # 如果record内有多个键，建立它们之间的关系\n",
    "        if len(keys) > 1:\n",
    "            for i, key1 in enumerate(keys):\n",
    "                val1 = record[key1]\n",
    "                if not val1:\n",
    "                    continue\n",
    "                label1 = fuzzy_node_type(key1)\n",
    "                id1 = f\"{label1}:{val1}\"\n",
    "                for j in range(i+1, len(keys)):\n",
    "                    val2 = record[keys[j]]\n",
    "                    if not val2:\n",
    "                        continue\n",
    "                    label2 = fuzzy_node_type(keys[j])\n",
    "                    id2 = f\"{label2}:{val2}\"\n",
    "\n",
    "                    # 建边，判断方向和关系\n",
    "                    rel, direction = get_relationship_and_direction(label1, label2)\n",
    "                    if direction == (label1, label2):\n",
    "                        if not G.has_edge(id1, id2):\n",
    "                            G.add_edge(id1, id2, label=rel)\n",
    "                    else:\n",
    "                        if not G.has_edge(id2, id1):\n",
    "                            G.add_edge(id2, id1, label=rel)\n",
    "\n",
    "    # --- Step 3: 建立 corrected_entities 和 context_result 之间的连接 ---\n",
    "\n",
    "    # 先统计已有的 PUBLISHED_IN_YEAR 和 PUBLISHED_IN_SOURCE 连接，避免重复\n",
    "    published_in_year_edges = set()\n",
    "    published_in_source_edges = set()\n",
    "    for u, v, attr in G.edges(data=True):\n",
    "        if attr.get('label') == \"PUBLISHED_IN_YEAR\":\n",
    "            published_in_year_edges.add((u,v))\n",
    "        if attr.get('label') == \"PUBLISHED_IN_SOURCE\":\n",
    "            published_in_source_edges.add((u,v))\n",
    "\n",
    "    # 遍历 corrected_entities 和 context_result 节点，做连接\n",
    "    # 这里用集合方便快速查找节点\n",
    "    context_nodes = {f\"{fuzzy_node_type(k)}:{v}\" for record in context_result for k,v in record.items() if v}\n",
    "    corrected_nodes = {f\"{k}:{v}\" for k, vals in corrected_entities.items() for v in vals}\n",
    "\n",
    "    # 先把所有节点放集合里方便判断\n",
    "    all_nodes = added_nodes\n",
    "\n",
    "    for ent_node in corrected_nodes:\n",
    "        ent_label, ent_name = ent_node.split(\":\", 1)\n",
    "        for ctx_node in context_nodes:\n",
    "            ctx_label, ctx_name = ctx_node.split(\":\", 1)\n",
    "            if ent_node == ctx_node:\n",
    "                continue  # 同一个节点不用连边\n",
    "\n",
    "            # 跳过没有在图中实际存在的节点\n",
    "            if ent_node not in all_nodes or ctx_node not in all_nodes:\n",
    "                continue\n",
    "\n",
    "            # 避免重复构建 PUBLISHED_IN_YEAR 和 PUBLISHED_IN_SOURCE\n",
    "            if (ent_label == \"Papers\" and ctx_label == \"Years\") or (ent_label == \"Years\" and ctx_label == \"Papers\"):\n",
    "                # 判断是否已存在对应边\n",
    "                if (ent_node, ctx_node) in published_in_year_edges or (ctx_node, ent_node) in published_in_year_edges:\n",
    "                    continue\n",
    "            if (ent_label == \"Papers\" and ctx_label == \"Sources\") or (ent_label == \"Sources\" and ctx_label == \"Papers\"):\n",
    "                if (ent_node, ctx_node) in published_in_source_edges or (ctx_node, ent_node) in published_in_source_edges:\n",
    "                    continue\n",
    "\n",
    "            rel, direction = get_relationship_and_direction(ent_label, ctx_label)\n",
    "            if direction == (ent_label, ctx_label):\n",
    "                if not G.has_edge(ent_node, ctx_node):\n",
    "                    G.add_edge(ent_node, ctx_node, label=rel)\n",
    "            else:\n",
    "                if not G.has_edge(ctx_node, ent_node):\n",
    "                    G.add_edge(ctx_node, ent_node, label=rel)\n",
    "\n",
    "    # --- Step 4: 如果有多个 Paper 节点，查询并添加引用关系 ---\n",
    "\n",
    "    paper_nodes = [n for n, d in G.nodes(data=True) if d.get(\"label\") == \"Papers\"]\n",
    "    if len(paper_nodes) > 1:\n",
    "        paper_titles = [G.nodes[p][\"name\"] for p in paper_nodes]\n",
    "        titles_list_str = \", \".join([f\"'{t}'\" for t in paper_titles])\n",
    "        cypher = f\"\"\"\n",
    "        MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "        WHERE p1.title IN [{titles_list_str}] AND p2.title IN [{titles_list_str}]\n",
    "        RETURN p1.title AS citing, p2.title AS cited\n",
    "        \"\"\"\n",
    "        cite_results = graph.run(cypher).data()\n",
    "        for row in cite_results:\n",
    "            citing = row.get(\"citing\")\n",
    "            cited = row.get(\"cited\")\n",
    "            id_citing = f\"Papers:{citing}\"\n",
    "            id_cited = f\"Papers:{cited}\"\n",
    "            if id_citing in G and id_cited in G and not G.has_edge(id_citing, id_cited):\n",
    "                G.add_edge(id_citing, id_cited, label=\"CITES\")\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "context_result = result['context']\n",
    "corrected_entities = result['entities'].dict()\n",
    "\n",
    "graph = Py2NeoGraph(uri, auth=(username, password))\n",
    "\n",
    "H = context_and_entities_to_graph(context_result, corrected_entities, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "26c14715-8a85-4c17-add9-31c2f26c62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图中的边数为: 48\n"
     ]
    }
   ],
   "source": [
    "num_edges = H.number_of_edges()\n",
    "print(f\"图中的边数为: {num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23d06f-26eb-4086-b1e4-cc9a6772bf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
