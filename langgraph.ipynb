{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da07437-1cdd-4808-9568-d5c007005576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict, List, Optional, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from py2neo import Graph as Py2NeoGraph\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "# External imports from your modules\n",
    "from graph_chain import build_graph_qa_chain\n",
    "from retriever_weaviate import retriever_weaviate\n",
    "from determine_database import final_answer\n",
    "from retriever_answer_async import partial_answer_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd51444-7a1f-4f59-ae60-f99bb3e361b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import (\n",
    "    determine_prompt,\n",
    "    recall_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517c64b7-ce14-4ad9-a6df-50ae1c583785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    question_type: Optional[str]\n",
    "    parsed_output: Dict\n",
    "    step: int\n",
    "    database: Optional[str]\n",
    "    strategy: Optional[str]\n",
    "    context_history: Dict[str, str]\n",
    "    retrieved_chunks: List[Dict]\n",
    "    sufficient: bool\n",
    "    llm: object\n",
    "    answer: Optional[str]\n",
    "    record: Optional[str]\n",
    "\n",
    "\n",
    "class QuestionAnalysis(BaseModel):\n",
    "    question_type: Literal[\"Knowledge-Type\", \"Entity-Type\", \"Mixed-Type\"]\n",
    "    database_to_call: Literal[\"Literature Text Database\", \"Literature Graph Database\", \"Both\"]\n",
    "    first_database_to_call: str\n",
    "    methods: Optional[str] = None\n",
    "    call_strategy: str\n",
    "\n",
    "\n",
    "class RecallDecision(BaseModel):\n",
    "    sufficient: bool\n",
    "    next_database: Optional[str] = None\n",
    "    reason: Optional[str] = None\n",
    "    strategy: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e66e58-9886-4e8e-9c0b-c47ea9a2b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_key = \"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\"\n",
    "wcd_url = \"https://yk7x0nnmqzuvgr8h2wu2sa.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
    "wcd_api_key = \"VqrACrlTS5xlfLNY2aAjHoLEI9RVU3EPDaMt\"\n",
    "\n",
    "uri = \"neo4j+s://c0abcb56.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = \"Vr5PhOR-n657dwRQsDfVWy_EYIE3QUUU59p7eOxJ39Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8d5954a-ffef-43dc-a7f4-d711a5fc0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-58c2a9a30bf74bc0bd69688acc27c83e\"  \n",
    "\n",
    "# 替换模型初始化\n",
    "llm = ChatOpenAI(model=\"qwen-turbo-1101\",\n",
    "                base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ab83a9-34a5-4896-84b1-782eb6cff8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_question_type(state: AgentState) -> AgentState:\n",
    "    prompt_template = ChatPromptTemplate.from_template(determine_prompt)\n",
    "    model = state['llm'].with_structured_output(QuestionAnalysis)\n",
    "    result = model.invoke(prompt_template.format_messages(question=state['question']))\n",
    "    state[\"question_type\"] = result.question_type\n",
    "    state[\"parsed_output\"] = result.dict()\n",
    "    state[\"database\"] = result.first_database_to_call if result.question_type == \"Mixed-Type\" else result.database_to_call\n",
    "    state[\"strategy\"] = result.call_strategy\n",
    "    return state\n",
    "\n",
    "\n",
    "def handle_entity_type(state: AgentState) -> AgentState:\n",
    "    graph = Py2NeoGraph(uri, auth=(\"neo4j\", password))\n",
    "    graph_qa_chain = build_graph_qa_chain(state['llm'], graph, entity_cache)\n",
    "    result = graph_qa_chain.invoke({\"question\": state[\"question\"]})\n",
    "    state[\"answer\"] = result\n",
    "    return state\n",
    "\n",
    "\n",
    "async def handle_knowledge_type(state: AgentState) -> AgentState:\n",
    "    all_result, final_response, _, _ = await partial_answer_async(\n",
    "        state[\"llm\"], state[\"question\"], wcd_url, wcd_api_key, huggingface_key\n",
    "    )\n",
    "    state[\"answer\"] = final_response\n",
    "    state[\"retrieved_chunks\"] = all_result\n",
    "    return state\n",
    "\n",
    "\n",
    "def graph_retrieval_node(state: AgentState) -> AgentState:\n",
    "    graph = Py2NeoGraph(uri, auth=(\"neo4j\", password))\n",
    "    graph_qa_chain = build_graph_qa_chain(state['llm'], graph, entity_cache)\n",
    "    result = graph_qa_chain.invoke({\"question\": f\"the entities needs to be searched{state['strategy']}\"})\n",
    "    state[\"context_history\"][f\"step {state['step']} from graph\"] = str(result)\n",
    "    state[\"retrieved_chunks\"] = result\n",
    "    return state\n",
    "\n",
    "def text_retrieval_node(state: AgentState) -> AgentState:\n",
    "    chunks = retriever_weaviate(state['strategy'], wcd_url, wcd_api_key, huggingface_key, limit=4)\n",
    "    context_list = [{'chunk': item['chunk'], 'title': item['title']} for item in chunks]\n",
    "    state[\"context_history\"][f\"step {state['step']} from text\"] = json.dumps(context_list)\n",
    "    state[\"retrieved_chunks\"] = chunks\n",
    "    return state\n",
    "\n",
    "\n",
    "def recall_decision_node(state: AgentState) -> AgentState:\n",
    "    prompt_template = ChatPromptTemplate.from_template(recall_prompt)\n",
    "    model = state['llm'].with_structured_output(RecallDecision)\n",
    "    context_str = json.dumps(state[\"context_history\"], indent=2)\n",
    "    result = model.invoke(\n",
    "        prompt_template.format_messages(\n",
    "            question=state[\"question\"],\n",
    "            context=context_str,\n",
    "            strategy=state[\"strategy\"]\n",
    "        )\n",
    "    )\n",
    "    state[\"sufficient\"] = result.sufficient\n",
    "    if not result.sufficient:\n",
    "        state[\"database\"] = result.next_database\n",
    "        state[\"strategy\"] = result.strategy\n",
    "    return state\n",
    "\n",
    "\n",
    "def final_answer_node(state: AgentState) -> AgentState:\n",
    "    state[\"answer\"] = final_answer(\n",
    "        state[\"llm\"], state[\"question\"], state[\"context_history\"], state[\"strategy\"], state.get(\"record\", \"\")\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def route_by_question_type(state: AgentState) -> str:\n",
    "    if state[\"question_type\"] == \"Entity-Type\":\n",
    "        return \"handle_entity_type\"\n",
    "    elif state[\"question_type\"] == \"Knowledge-Type\":\n",
    "        return \"handle_knowledge_type\"\n",
    "    else:\n",
    "        return \"graph_retrieval\" if state[\"database\"] == \"Literature Graph Database\" else \"text_retrieval\"\n",
    "\n",
    "\n",
    "def route_after_recall(state: AgentState) -> str:\n",
    "    if state[\"sufficient\"]:\n",
    "        return \"final_answer\"\n",
    "    else:\n",
    "        return \"graph_retrieval\" if state[\"database\"] == \"Literature Graph Database\" else \"text_retrieval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7dff10-7881-46cc-b159-9a52b3385769",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"entity_cache.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    entity_cache = json.load(f)\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"determine_question_type\", determine_question_type)\n",
    "builder.add_node(\"handle_entity_type\", handle_entity_type)\n",
    "builder.add_node(\"handle_knowledge_type\", handle_knowledge_type)\n",
    "builder.add_node(\"graph_retrieval\", graph_retrieval_node)\n",
    "builder.add_node(\"text_retrieval\", text_retrieval_node)\n",
    "builder.add_node(\"recall_decision\", recall_decision_node)\n",
    "builder.add_node(\"final_answer\", final_answer_node)\n",
    "\n",
    "builder.set_entry_point(\"determine_question_type\")\n",
    "builder.add_conditional_edges(\"determine_question_type\", route_by_question_type)\n",
    "builder.add_edge(\"handle_entity_type\", END)\n",
    "builder.add_edge(\"handle_knowledge_type\", END)\n",
    "builder.add_edge(\"graph_retrieval\", \"recall_decision\")\n",
    "builder.add_edge(\"text_retrieval\", \"recall_decision\")\n",
    "builder.add_conditional_edges(\"recall_decision\", route_after_recall)\n",
    "builder.add_edge(\"final_answer\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "async def agent(llm, question: str, record: str = \"\"):\n",
    "    initial_state: AgentState = {\n",
    "        \"question\": question,\n",
    "        \"parsed_output\": {},\n",
    "        \"step\": 1,\n",
    "        \"database\": None,\n",
    "        \"strategy\": None,\n",
    "        \"context_history\": {},\n",
    "        \"retrieved_chunks\": [],\n",
    "        \"sufficient\": False,\n",
    "        \"llm\": llm,\n",
    "        \"record\": record,\n",
    "        \"answer\": None,\n",
    "        \"question_type\": None,\n",
    "    }\n",
    "    result = await graph.ainvoke(initial_state)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9297697a-05af-4b1f-830a-1804a47c2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supabase\n",
      "  Downloading supabase-2.17.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gotrue==2.12.3 (from supabase)\n",
      "  Downloading gotrue-2.12.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.26 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from supabase) (0.27.2)\n",
      "Collecting postgrest==1.1.1 (from supabase)\n",
      "  Downloading postgrest-1.1.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting realtime==2.6.0 (from supabase)\n",
      "  Downloading realtime-2.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting storage3==0.12.0 (from supabase)\n",
      "  Downloading storage3-0.12.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting supafunc==0.10.1 (from supabase)\n",
      "  Downloading supafunc-0.10.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from gotrue==2.12.3->supabase) (2.9.2)\n",
      "Collecting pyjwt<3.0.0,>=2.10.1 (from gotrue==2.12.3->supabase)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from postgrest==1.1.1->supabase) (2.1.0)\n",
      "Collecting strenum<0.5.0,>=0.4.9 (from postgrest==1.1.1->supabase)\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.10 (from gotrue==2.12.3->supabase)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting typing-extensions>=4.14.0 (from realtime==2.6.0->supabase)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: websockets<16,>=11 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from realtime==2.6.0->supabase) (13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from storage3==0.12.0->supabase) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (1.0.6)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.14.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from deprecation<3.0.0,>=2.1.0->postgrest==1.1.1->supabase) (24.1)\n",
      "Collecting h2<5,>=3 (from httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase)\n",
      "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from pydantic<3,>=1.10->gotrue==2.12.3->supabase) (0.7.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10->gotrue==2.12.3->supabase)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10->gotrue==2.12.3->supabase)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->storage3==0.12.0->supabase) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.2.2)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase)\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase)\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading supabase-2.17.0-py3-none-any.whl (17 kB)\n",
      "Downloading gotrue-2.12.3-py3-none-any.whl (44 kB)\n",
      "Downloading postgrest-1.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading realtime-2.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading storage3-0.12.0-py3-none-any.whl (18 kB)\n",
      "Downloading supafunc-0.10.1-py3-none-any.whl (8.0 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: strenum, typing-extensions, pyjwt, hyperframe, hpack, typing-inspection, pydantic-core, h2, pydantic, realtime, supafunc, storage3, postgrest, gotrue, supabase\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: pyjwt\n",
      "    Found existing installation: PyJWT 2.8.0\n",
      "    Uninstalling PyJWT-2.8.0:\n",
      "      Successfully uninstalled PyJWT-2.8.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.23.4\n",
      "    Uninstalling pydantic_core-2.23.4:\n",
      "      Successfully uninstalled pydantic_core-2.23.4\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.9.2\n",
      "    Uninstalling pydantic-2.9.2:\n",
      "      Successfully uninstalled pydantic-2.9.2\n",
      "Successfully installed gotrue-2.12.3 h2-4.2.0 hpack-4.1.0 hyperframe-6.1.0 postgrest-1.1.1 pydantic-2.11.7 pydantic-core-2.33.2 pyjwt-2.10.1 realtime-2.6.0 storage3-0.12.0 strenum-0.4.15 supabase-2.17.0 supafunc-0.10.1 typing-extensions-4.14.1 typing-inspection-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (d:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (d:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\~ydantic_core'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution -cipy (d:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.20 requires langsmith<0.4,>=0.1.17, but you have langsmith 0.4.8 which is incompatible.\n",
      "langchain-community 0.3.5 requires langsmith<0.2.0,>=0.1.125, but you have langsmith 0.4.8 which is incompatible.\n",
      "tabled-pdf 0.1.4 requires scikit-learn<2.0.0,>=1.5.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "zhipuai 2.1.5.20230904 requires pyjwt<2.9.0,>=2.8.0, but you have pyjwt 2.10.1 which is incompatible.\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\utils\\_process_win32.py:124: ResourceWarning: unclosed file <_io.BufferedWriter name=3>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\utils\\_process_win32.py:124: ResourceWarning: unclosed file <_io.BufferedReader name=4>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\utils\\_process_win32.py:124: ResourceWarning: unclosed file <_io.BufferedReader name=5>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cd5ba-84ca-410e-9684-bc44a26ef286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
